# -*- coding: utf-8 -*-
"""hw4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/142Hy3tEXnj1azg5htTo74BqK9ZmBl6Et

#Naive Bayes Classification and Clustering

##Part 1: Naïve Bayes Classification
"""

import pandas as pd
naive_train_data = pd.read_csv('/content/hw4_naive.csv')

# preview train data
print(naive_train_data.shape)
print(naive_train_data.columns)
print(naive_train_data.head())
print(naive_train_data)

from sklearn.model_selection import train_test_split

target = naive_train_data['Label']
naive_train_data = naive_train_data.drop(columns=['Label'])
features = ['Feature_1', 'Feature_2', 'Feature_3', 'Feature_4', 'Feature_5', 'Feature_6']
feature_indices = [0, 1, 2, 3, 4, 5]

# split the data
x_train, x_test, y_train, y_test = train_test_split(naive_train_data, target, test_size=0.2, random_state=42)

# Multinomial Naïve Bayes Classifier with Smoothing

# k is the total number of classes
k = 2
# x_i is the name of the column feature we're looking for values in
# v is the value for a specific feature column
# c is a class label
def count_feature_value(x_i, v, c, x_train, y_train):
  counter = 0
  for row, class_label in zip(x_train.itertuples(index=True, name='Pandas'), y_train):
    if (getattr(row, x_i) == v) and (class_label == c):
      counter += 1
  return counter

def count_class_label(c, y_train):
  counter = 0
  for class_label in y_train:
    if (class_label == c):
      counter += 1
  return counter

def feature_value_probability_given_class_label(x_i, v, c, k, x_train, y_train):
  probability = count_feature_value(x_i, v, c, x_train, y_train) + 1
  probability /= (count_class_label(c, y_train) + k)
  return probability

def class_label_probability(c, y_train):
  counter = count_class_label(c, y_train)
  return counter /len(y_train.index)

def mnb_with_smoothing(row, c, k, x_train, y_train):
  c_label_probability = 1
  for feature, feature_index in zip(features, feature_indices):
    c_label_probability = c_label_probability * feature_value_probability_given_class_label(feature, row[feature_index], c, k, x_train, y_train)
  c_label_probability = c_label_probability * class_label_probability(c, y_train)
  return c_label_probability

def mnb_predict_label(row, x_train, y_train):
  class_0_probability = mnb_with_smoothing(row, 0, k, x_train, y_train)
  class_1_probability = mnb_with_smoothing(row, 1, k, x_train, y_train)
  if class_0_probability >= class_1_probability:
    return 0
  else:
    return 1

# predict labels
data_predictions = []

for index, row in x_test.iterrows():
  label = mnb_predict_label(row, x_train, y_train)
  data_predictions.append(label)

y_predictions_multinomial_naive_bayes = pd.DataFrame(data_predictions)

# Gaussian Naïve Bayes Classifier w/out Smoothing
import statistics
import math

def calculate_mean_c(x_i, c, x_train, y_train):
  data_list = []
  for row, class_label in zip(x_train.itertuples(index=True, name='Pandas'), y_train):
    if class_label == c:
      data_list.append(getattr(row, x_i))
  return statistics.mean(data_list)

def calculate_standard_deviation_c(x_i, c, x_train, y_train):
  data_list = []
  for row, class_label in zip(x_train.itertuples(index=True, name='Pandas'), y_train):
    if class_label == c:
      data_list.append(getattr(row, x_i))
  return statistics.stdev(data_list)

def gnb_without_smoothing(x_i, v, c, k, x_train, y_train):
  probability = 0
  mean_c = calculate_mean_c(x_i, c, x_train, y_train)
  standard_deviation_c = calculate_standard_deviation_c(x_i, c, x_train, y_train)
  probability = 1/math.sqrt(2 * math.pi * (standard_deviation_c**2))
  probability = probability * math.e ** (((-1)*((v - mean_c)**2))/(2*(standard_deviation_c**2)))
  return probability

def gaussian_naive_bayes(row, c, k, x_train, y_train):
  c_label_probability = 1
  for feature, feature_index in zip(features, feature_indices):
    c_label_probability = c_label_probability * gnb_without_smoothing(feature, row[feature_index], c, k, x_train, y_train)
  c_label_probability = c_label_probability * class_label_probability(c, y_train)
  return c_label_probability

def gnb_predict_label(row, x_train, y_train):
  class_0_probability = gaussian_naive_bayes(row, 0, k, x_train, y_train)
  class_1_probability = gaussian_naive_bayes(row, 1, k, x_train, y_train)
  if class_0_probability >= class_1_probability:
    return 0
  else:
    return 1

# predict labels
data_predictions = []

for index, row in x_test.iterrows():
  label = gnb_predict_label(row, x_train, y_train)
  data_predictions.append(label)

y_predictions_gaussian_naive_bayes = pd.DataFrame(data_predictions)

# Calculate the accuracy and the F1 score of test data using both models
from sklearn.metrics import accuracy_score, f1_score

def evaluate_model (y_actual, y_pred):
  accuracy_score_metric = accuracy_score(y_actual, y_pred)
  f1_score_metric = f1_score(y_actual, y_pred, average='macro')
  print("   Accuray Score is", accuracy_score_metric)
  print("   Macro F1 Score is", f1_score_metric)

print("Multinomial Naive Bayes Model with Laplace Smoothing:")
evaluate_model(y_predictions_multinomial_naive_bayes, y_test)

print("")
print("Gaussian Naïve Bayes Model without Smoothing:")
evaluate_model(y_predictions_gaussian_naive_bayes, y_test)

"""##Part 2: Clustering"""

import pandas as pd
cluster_train_data = pd.read_csv('/content/hw4_cluster.csv')

# preview train data
print(cluster_train_data.shape)
print(cluster_train_data.columns)
cluster_train_data.head()

class Cluster:
  def __init__(self, cluster_elements, centroid_calculation_method, data_dimension):
    self.cluster_elements = cluster_elements
    self.cluster_centroid = centroid(cluster_elements, centroid_calculation_method, data_dimension)

import statistics
def centroid(cluster_elements, centroid_calculation_method, data_dimension):
  mean_centroid = []
  if (centroid_calculation_method == 'mean'):
    if cluster_elements == []:
      for i in range(data_dimension):
        mean_centroid.append(0)
    else:
      for i in range(data_dimension):
        dimension_i_values = []
        for element in cluster_elements:
          dimension_i_values.append(element[i])
        mean_centroid.append(statistics.mean(dimension_i_values))
  return mean_centroid

import random
def random_seed_selection(train_data, k, centroid_calculation_method, data_dimension):
  set_of_clusters = []
  cluster_elements = []
  row_count = len(train_data)

  # convert the train_data data frame into a list of elements
  list_of_elements = []
  for index, row in train_data.iterrows():
    element = [row['x1'], row['x2']]
    list_of_elements.append(element)

  # randonly assign k seeds into k buckets
  for iteration in range(k):
    index = random.randint(0, len(list_of_elements)-1)
    element = list_of_elements.pop(index)
    cluster_elements.append(element)
    cluster = Cluster(cluster_elements, centroid_calculation_method, data_dimension)
    set_of_clusters.append(cluster)
    cluster_elements = []

  return set_of_clusters, list_of_elements

def random_split_initialization(train_data, k, centroid_calculation_method, data_dimension):
  set_of_clusters = []
  cluster_elements = []
  row_count = len(train_data)
  elements_per_cluster = row_count//k

  # convert the train_data data frame into a list of elements
  list_of_elements = []
  for index, row in train_data.iterrows():
    element = [row['x1'], row['x2']]
    list_of_elements.append(element)

  # randonly assign elements into k buckets
  cluster_index = 1
  for iteration in range(row_count):
    index = random.randint(0, len(list_of_elements)-1)
    element = list_of_elements.pop(index)
    cluster_elements.append(element)
    if len(cluster_elements) == elements_per_cluster and cluster_index < k:
      cluster = Cluster(cluster_elements, centroid_calculation_method, 2)
      set_of_clusters.append(cluster)
      cluster_elements = []
      cluster_index += 1

  cluster = Cluster(cluster_elements, centroid_calculation_method, data_dimension)
  set_of_clusters.append(cluster)
  return set_of_clusters

def cost(element, cluster, data_dimension):
  distance = 0
  for dimension in range(data_dimension):
    distance += (element[dimension] - cluster.cluster_centroid[dimension]) ** 2
  return distance

def cluster_minimal_cost(element, set_of_clusters, data_dimension):
  minimum_distance = cost(element, set_of_clusters[0], data_dimension)
  cluster_index_with_minimum_cost = 0
  for i in range (len(set_of_clusters)):
    distance = cost(element, set_of_clusters[i], data_dimension)
    if distance < minimum_distance:
      minimum_distance = distance
      cluster_index_with_minimum_cost = i
  return cluster_index_with_minimum_cost

# implement a generalized K-means algorithm
def k_means(data_points, k, centroid_calculation_method, initialization_method, max_iter):
  # calculate the dimension of data points we are dealing with
  data_dimension = len(data_points.columns)

  if initialization_method == random_split_initialization:
    # initializa the k buckets
    set_of_clusters = initialization_method(data_points, k, centroid_calculation_method, data_dimension)
    # start moving elements around
    current_iter = 0
    cluster_change_made = True
    while (current_iter < max_iter or cluster_change_made):
      cluster_change_made = False

      # for each element s in each cluster
      for cluster, cluster_index in zip(set_of_clusters, range(len(set_of_clusters))):
        for element, element_index in zip(cluster.cluster_elements, range(len(cluster.cluster_elements))):
          # find the bucket Si such that s-Si distance is minimal
          cluster_index_with_minimum_cost = cluster_minimal_cost(element, set_of_clusters, data_dimension)

          if cluster_index_with_minimum_cost != cluster_index:
            # put s in that bucket
            set_of_clusters[cluster_index_with_minimum_cost].cluster_elements.append(element)
            # remove s from the bucket it was in
            set_of_clusters[cluster_index].cluster_elements.pop(element_index)
            # update the boolean value of the cluster change variable
            cluster_change_made = True

      # update the mean of each cluster
      for cluster, cluster_index in zip (set_of_clusters, range(len(set_of_clusters))):
        new_cluster = Cluster(cluster.cluster_elements, centroid_calculation_method, data_dimension)
        set_of_clusters[cluster_index] = new_cluster

      # increase iteration number
      current_iter += 1

  elif initialization_method == random_seed_selection:
    # initializa the k buckets
    set_of_clusters, list_of_elements = initialization_method(data_points, k, centroid_calculation_method, data_dimension)
    # start adding new elements to the buckets
    current_iter = 0
    cluster_change_made = True

    # for each element that has not been assigned to a cluster yet
    for element in list_of_elements:
        # find the bucket Si such that s-Si distance is minimal
        cluster_index_with_minimum_cost = cluster_minimal_cost(element, set_of_clusters, data_dimension)
        # insert that element to the cluster
        set_of_clusters[cluster_index_with_minimum_cost].cluster_elements.append(element)

    # update the mean of each cluster
    for cluster, cluster_index in zip (set_of_clusters, range(len(set_of_clusters))):
      new_cluster = Cluster(cluster.cluster_elements, centroid_calculation_method, data_dimension)
      set_of_clusters[cluster_index] = new_cluster

    # increase iteration number
    current_iter += 1

    # keep repeating the process until the stopping condition is true
    while (current_iter < max_iter or cluster_change_made):
      cluster_change_made = False

      # for each element s in each cluster
      for cluster, cluster_index in zip(set_of_clusters, range(len(set_of_clusters))):
        for element, element_index in zip(cluster.cluster_elements, range(len(cluster.cluster_elements))):
          # find the bucket Si such that s-Si distance is minimal
          cluster_index_with_minimum_cost = cluster_minimal_cost(element, set_of_clusters, data_dimension)

          if cluster_index_with_minimum_cost != cluster_index:
            # put s in that bucket
            set_of_clusters[cluster_index_with_minimum_cost].cluster_elements.append(element)
            # remove s from the bucket it was in
            set_of_clusters[cluster_index].cluster_elements.pop(element_index)
            # update the boolean value of the cluster change variable
            cluster_change_made = True

      # update the mean of each cluster
      for cluster, cluster_index in zip (set_of_clusters, range(len(set_of_clusters))):
        new_cluster = Cluster(cluster.cluster_elements, centroid_calculation_method, data_dimension)
        set_of_clusters[cluster_index] = new_cluster

      # increase iteration number
      current_iter += 1

  return set_of_clusters

# implement a function from scratch that calculates the silhouette score for a list of clusters
from math import sqrt
def distance(data_point_1, data_point_2):
  distance_value = 0
  data_dimension = len(data_point_1)
  for i in range(data_dimension):
    distance_value += (data_point_1[i] - data_point_2[i]) ** 2
  distance_value = sqrt(distance_value)
  return distance_value

def nearest_cluster(data_point, set_of_clusters, data_point_cluster_index):
  if (data_point_cluster_index == 0):
    minimum_distance = distance(data_point, set_of_clusters[1].cluster_centroid)
    nearest_cluster_index = 1
  else:
    minimum_distance = distance(data_point, set_of_clusters[0].cluster_centroid)
    nearest_cluster_index = 0

  for i in range (len(set_of_clusters)):
    if i != data_point_cluster_index:
      dist = distance(data_point, set_of_clusters[i].cluster_centroid)
      if dist < minimum_distance:
        minimum_distance = dist
        nearest_cluster_index = i
  return nearest_cluster_index

def silhouette_score_per_data_point(data_point, set_of_clusters, data_point_cluster_index):
  a = 0
  b = 0
  # calculate a
  for element in set_of_clusters[data_point_cluster_index].cluster_elements:
    if element != data_point:
      a += distance(data_point, element)
  a = a/(len(set_of_clusters[data_point_cluster_index].cluster_elements))

  # find the index of the nearest cluster
  nearest_cluster_index = nearest_cluster(data_point, set_of_clusters, data_point_cluster_index)

  # calculate b
  for element in set_of_clusters[nearest_cluster_index].cluster_elements:
    if element != data_point:
      b += distance(data_point, element)
  b = b/(len(set_of_clusters[nearest_cluster_index].cluster_elements))

  # calculate silhouette score for this data point
  score = b - a
  score = score / (max(a, b))
  return score

def silhouette_score(set_of_clusters):
  score = 0
  data_points_count = 0
  for data_point_cluster_index in range(len(set_of_clusters)):
    for data_point in set_of_clusters[data_point_cluster_index].cluster_elements:
      score += silhouette_score_per_data_point(data_point, set_of_clusters, data_point_cluster_index)
      data_points_count += 1
  score = score / data_points_count
  return score

"""###Random Seed Selection Test Case"""

# random seed selection test case
k = 5
centroid_calculation_method = 'mean'
initialization_method = random_seed_selection
max_iter = 50
set_of_clusters = k_means(cluster_train_data, k, centroid_calculation_method, initialization_method, max_iter)
print("test case when")
print("   k=", k)
print("   initialization method is random seed selection")
print("   centroid calculation method is ", centroid_calculation_method)
print("   max iterations is ", max_iter)
print("silhouette score: ", silhouette_score(set_of_clusters))

"""###Random Split Initialization Test Cases"""

centroid_calculation_method = 'mean'
initialization_method = random_split_initialization
max_iter = 100

# case 1
k = 2
set_of_clusters = k_means(cluster_train_data, k, centroid_calculation_method, initialization_method, max_iter)
print("test case when")
print("   k=", k)
print("   initialization method is split random initialization")
print("   centroid calculation method is ", centroid_calculation_method)
print("   max iterations is ", max_iter)
print("silhouette score: ", silhouette_score(set_of_clusters))
print("-------------------------------------------------------")

# case 2
k = 3
set_of_clusters = k_means(cluster_train_data, k, centroid_calculation_method, initialization_method, max_iter)
print("test case when")
print("   k=", k)
print("   initialization method is split random initialization")
print("   centroid calculation method is ", centroid_calculation_method)
print("   max iterations is ", max_iter)
print("silhouette score: ", silhouette_score(set_of_clusters))
print("-------------------------------------------------------")

# case 3
k = 4
set_of_clusters = k_means(cluster_train_data, k, centroid_calculation_method, initialization_method, max_iter)
print("test case when")
print("   k=", k)
print("   initialization method is split random initialization")
print("   centroid calculation method is ", centroid_calculation_method)
print("   max iterations is ", max_iter)
print("silhouette score: ", silhouette_score(set_of_clusters))
print("-------------------------------------------------------")

# case 4
k = 5
set_of_clusters = k_means(cluster_train_data, k, centroid_calculation_method, initialization_method, max_iter)
print("test case when")
print("   k=", k)
print("   initialization method is split random initialization")
print("   centroid calculation method is ", centroid_calculation_method)
print("   max iterations is ", max_iter)
print("silhouette score: ", silhouette_score(set_of_clusters))

"""after calculating the silhouette score for the 4 test cases, we conclude that the best k is when k = 2"""